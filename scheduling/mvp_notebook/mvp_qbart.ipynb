{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QBART: Minimal Viable Product Edition\n",
    "\n",
    "*Welcome to QBART, the Quantized, Bitserial, AcceleRaThor!*\n",
    "\n",
    "<img src=\"logo.png\",width=400,height=400>\n",
    "\n",
    "In this MVP-implementation, the QBART-team have prepared the following:\n",
    "- Two layers run on the FPGA (the actual accelerator): thresholding and fully connected. A padding unit is also available for the several layers.\n",
    "- All the other layers run on the Cortex A9s: pooling, convolution, sliding window.\n",
    "- We utilize little to no BRAM on the FPGA, as most IO is saved directly to DRAM, and we have no custom memory hierarchy for the FPGA, so memory performance is suboptimal.\n",
    "- We use the GTSRB-benchmark as the default in testing.\n",
    "\n",
    "All in all, it might not accelerate anything at all, so the MVP is more a proof of concept, while future iterations will actually make this faster than the actual implementation. \n",
    "\n",
    "Alright, let's get to it!\n",
    "\n",
    "## Requirements:\n",
    "- A trained QNN that is pickled and formatted similarly as the GTSRB benchmark.\n",
    "- This must be placed on the PYNQ, and you must edit the QNN path below so that QBART can find and work on it.\n",
    "- Image(s) must also be placed in a seperate folders, and you must set the image path accordingly.\n",
    "\n",
    "Alright, with the requirements done, we do the following:\n",
    "1. Run all image classifications on QBART, and time it.\n",
    "2. Run all image classifications on a pure, correct CPU implementation, and time it.\n",
    "3. Check if both QBART and the CPU implementation agree. If both implementations agree on all image classifications, we know that the QBART implementation is correct.\n",
    "4. Present the results to the user.\n",
    "\n",
    "\n",
    "\n",
    "TODO(N35N0M): Implement Yaman's GEMMBitserial as a separate CPU-based implementation. Does the common-case (matrix multiplication) very fast. We like fast.\n",
    "\n",
    "TODO(N35N0M): Currently, it seems like at every run of the code, we create an additonal root logger. Fix the code so that it either checks for an existing logger at a new runthrough, or that we safely destruct all loggers before running again.\n",
    "\n",
    "TODO: We should specify how the input QNNs should be constructed, ask Yaman?\n",
    "Is nice to have if we actually want others to be able to use QBART later.\n",
    "\n",
    "TODO: Fix logger so that it actually works. Should be a helper file in qbart_helper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Running all image classifications on QBART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It is a jpg!\n",
      "It is a jpg!\n",
      "It is a jpg!\n",
      "It is a jpg!\n",
      "603431\n",
      "00000000000010010011010100100111\n",
      "16902\n",
      "('Size of image list that is now being sent:', '00000000000000000100001000000110')\n",
      "The image list has been sent\n",
      "603431\n",
      "00000000000010010011010100100111\n",
      "16964\n",
      "('Size of image list that is now being sent:', '00000000000000000100001001000100')\n",
      "The image list has been sent\n",
      "Turn left ahead\n",
      "Stop\n",
      "50 Km/h\n",
      "Turn right ahead\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "CREDIT WHERE CREDIT IS DUE\n",
    "Parts of the code is simply reuse of the tutorial code provided by the course instructors.\n",
    "Should be available at: https://github.com/maltanar/qnn-inference-examples.\n",
    "\n",
    "Some python specific problems have been solved with stackoverflow help, \n",
    "and this is clearly credited in relevant code sections.\n",
    "\"\"\"\n",
    "\n",
    "# Open source libraries\n",
    "import logging\n",
    "\n",
    "\n",
    "# Custom functions for the project\n",
    "from qbart_helper import *\n",
    "from QNN import *\n",
    "from client import classification_client\n",
    "\n",
    "###########################################################################################################\n",
    "### USER INPUT SECTION, USER MUST SUBMIT VALUES OR \"None\" WHERE APPLICABLE\n",
    "###########################################################################################################\n",
    "\n",
    "\n",
    "qnn_path = \"gtsrb-w1a1.pickle\"         # Image directory, relative to where the notebook resides.\n",
    "image_dir = \"gtsrb_images\"             # Image directory, relative to where the notebook resides.\n",
    "image_limit = 100                      # Max amount of images to be inferenced, set None to inference all.\n",
    "image_channels = \"RGB\"                 # Must be specified in order. 'R', 'G' and 'B' combinations only.\n",
    "image_data_layout = \"rcC\"              # Must be specified, r = row, c = column, C = Channel\n",
    "\n",
    "qbart_data_layout = \"Crc\"              # Qbart assumes data to be in column major form.\n",
    "\n",
    "qnn_trained_channels = \"BGR\"           # The channel ordering that the qnn is trained to.\n",
    "qnn_trained_imsize_col = 32            # The expected column size of input images to the qnn.\n",
    "qnn_trained_imsize_row = 32            # The expected row size of input images to the qnn.\n",
    "\n",
    "# Cluster config\n",
    "solfrid = '192.168.1.5'\n",
    "server_list = [('localhost', 64646),(solfrid, 64646)]\n",
    "\n",
    "# Either specify image classes to get an easily readable name, or specify None to just get a category #.\n",
    "image_classes = ['20 Km/h', '30 Km/h', '50 Km/h', '60 Km/h', '70 Km/h', '80 Km/h', 'End 80 Km/h', '100 Km/h', '120 Km/h', 'No overtaking', 'No overtaking for large trucks', 'Priority crossroad', 'Priority road', 'Give way', 'Stop', 'No vehicles', 'Prohibited for vehicles with a permitted gross weight over 3.5t including their trailers, and for tractors except passenger cars and buses', 'No entry for vehicular traffic', 'Danger Ahead', 'Bend to left', 'Bend to right', 'Double bend (first to left)', 'Uneven road', 'Road slippery when wet or dirty', 'Road narrows (right)', 'Road works', 'Traffic signals', 'Pedestrians in road ahead', 'Children crossing ahead', 'Bicycles prohibited', 'Risk of snow or ice', 'Wild animals', 'End of all speed and overtaking restrictions', 'Turn right ahead', 'Turn left ahead', 'Ahead only', 'Ahead or right only', 'Ahead or left only', 'Pass by on right', 'Pass by on left', 'Roundabout', 'End of no-overtaking zone', 'End of no-overtaking zone for vehicles with a permitted gross weight over 3.5t including their trailers, and for tractors except passenger cars and buses']\n",
    "\n",
    "###########################################################################################################\n",
    "###########################################################################################################\n",
    "\n",
    "###########################################################################################################\n",
    "### MAIN METHOD, SHOULD BE KEPT RELATIVELY SIMPLE, DETAILS STORED AWAY IN HELPER FUNCTIONS\n",
    "###########################################################################################################\n",
    "images = load_images(image_dir, image_limit, qnn_trained_imsize_col, qnn_trained_imsize_row, qbart_data_layout, qnn_trained_channels)\n",
    "qnn = load_qnn(qnn_path)\n",
    "\n",
    "# We send the images to the processing server (currently localhost, can later be localhost and others (each with\n",
    "# its separate thread here in main or in classification client.))\n",
    "qbart_classifications = classification_client(qnn, images, server_list)\n",
    "qbart_classifications = [j for i in qbart_classifications for j in i]\n",
    "\n",
    "\n",
    "# Remember that we are just executing a QNN, so mispredictions is no indicator of failure/success.\n",
    "# Classifications are only for us so one can test that qbart actually runs properly.\n",
    "# This means that qbart_classification alone tells us very little, we need one or several correct cpu-implementations\n",
    "# to compare to.\n",
    "for i in range(len(qbart_classifications)):\n",
    "    print(image_classes[qbart_classifications[i]])\n",
    "    \n",
    "###########################################################################################################\n",
    "###########################################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Running all image classifications on a CPU implementation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Using the code from qnn-inference-examples (GTSRB only)\n",
    "With some mods and assumptions in order to process alot of images instead of just one.\n",
    "This also required some modding of the providedGTSRB_predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cPickle as pickle\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from QNN import *\n",
    "from time import time\n",
    "from QNN.layers import *\n",
    "from qbart_helper import *\n",
    "\n",
    "gtsrb_classes = ['20 Km/h', '30 Km/h', '50 Km/h', '60 Km/h', '70 Km/h', '80 Km/h', 'End 80 Km/h', '100 Km/h', '120 Km/h', 'No overtaking', 'No overtaking for large trucks', 'Priority crossroad', 'Priority road', 'Give way', 'Stop', 'No vehicles', 'Prohibited for vehicles with a permitted gross weight over 3.5t including their trailers, and for tractors except passenger cars and buses', 'No entry for vehicular traffic', 'Danger Ahead', 'Bend to left', 'Bend to right', 'Double bend (first to left)', 'Uneven road', 'Road slippery when wet or dirty', 'Road narrows (right)', 'Road works', 'Traffic signals', 'Pedestrians in road ahead', 'Children crossing ahead', 'Bicycles prohibited', 'Risk of snow or ice', 'Wild animals', 'End of all speed and overtaking restrictions', 'Turn right ahead', 'Turn left ahead', 'Ahead only', 'Ahead or right only', 'Ahead or left only', 'Pass by on right', 'Pass by on left', 'Roundabout', 'End of no-overtaking zone', 'End of no-overtaking zone for vehicles with a permitted gross weight over 3.5t including their trailers, and for tractors except passenger cars and buses']\n",
    "\n",
    "\n",
    "\n",
    "# Here we assume that the images are in channel, row, column layout, in BGR color.\n",
    "#tutorial_classifications = []\n",
    "\n",
    "#tutorial_start = time()\n",
    "#for image in images:\n",
    "#    tutorial_classifications.append(GTSRB_predict(image))\n",
    "    \n",
    "#tutorial_stop = time()\n",
    "\n",
    "#tutorial_time_total = tutorial_stop - tutorial_start\n",
    "\n",
    "# Tutorial code galore.\n",
    "def prepare_gtsrb(img):\n",
    "    # make sure the image is the size expected by the network\n",
    "    img = img.resize((32, 32))\n",
    "    display(img)\n",
    "    # convert to numpy array\n",
    "    img = np.asarray(img)\n",
    "    # we need the data layout to be (channels, rows, columns)\n",
    "    # but it comes in (rows, columns, channels) format, so we\n",
    "    # need to transpose the axes:\n",
    "    img = img.transpose((2, 0, 1))\n",
    "    # finally, our network is trained with BGR instead of RGB images,\n",
    "    # so we need to invert the order of channels in the channel axis:\n",
    "    img = img[::-1, :, :]\n",
    "    return img\n",
    "\n",
    "# load test images and prepare them\n",
    "\n",
    "qnn = pickle.loads(load_qnn(\"gtsrb-w1a1.pickle\"))\n",
    "\n",
    "\n",
    "def gtsrb_predict(img):\n",
    "    # get the predictions array\n",
    "    res = predict(qnn, img)\n",
    "    # return the index of the largest prediction, then use the\n",
    "    # classes array to map to a human-readable string\n",
    "    winner_ind = np.argmax(res)\n",
    "    winner_class = gtsrb_classes[winner_ind]\n",
    "    # the sum of the output values add up to 1 due to softmax,\n",
    "    # so we can interpret them as probabilities\n",
    "    return winner_class\n",
    "\n",
    "qnn_classifications = []\n",
    "qnn_classifications.append(gtsrb_predict(images[0]))\n",
    "qnn_classifications.append(gtsrb_predict(images[1]))\n",
    "qnn_classifications.append(gtsrb_predict(images[2]))\n",
    "qnn_classifications.append(gtsrb_predict(images[3]))\n",
    "\n",
    "print (qnn_classifications)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Using Yaman's GEMMBITserial implementation (much faster?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO(N35N0M): Implement this, and time it. - https://github.com/maltanar/gemmbitserial\n",
    "# C++ library, which means we have to do the following:\n",
    "# Mod the most relevant layers that use matrix multiplication (hint hint convolution and fc hinthint)\n",
    "# ??\n",
    "# Great success\n",
    "\n",
    "# Haven't done this before, but several suggestions can be found in: http://intermediate-and-advanced-software-carpentry.readthedocs.io/en/latest/c++-wrapping.html\n",
    "# \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Step 3: Implementation correctness testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(qbart_classifications)\n",
    "print(qnn_classifications)\n",
    "\n",
    "if qnn_classifications == qbart_classifications:\n",
    "    print(\"Holy Glomgold! It works!\")\n",
    "    # TODO: Present time used here. Perhaps energy later as well?\n",
    "else:\n",
    "    print(\"Uh-oh, something must'ave gone wrong somewhere!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Step 4: Presentation of results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Present results. The most important one here is just presenting the various times.\n",
    "# Not important for MVP rly, but if there is time (HA!), please fix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
