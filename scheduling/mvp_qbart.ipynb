{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QBART: A general QNN inference Accelerator\n",
    "\n",
    "*Welcome to QBART, the Quantized, Bitserial, AcceleRaTor!*\n",
    "\n",
    "<img src=\"logo.png\",width=400,height=400>\n",
    "\n",
    "In this MVP-implementation, the QBART-team have prepared the following:\n",
    "- Three layers run on the FPGA: Thresholding, Fully Connected, and Convolution.\n",
    "- All the other layers run on the Cortex A9s through this notebook: pooling, minimax, and ??\n",
    "- We utilize little to no BRAM on the FPGA, as most IO is saved directly to DRAM, and we have no custom memory hierarchy for the FPGA, so memory performance is suboptimal.\n",
    "- We use the GTSRB-benchmark as the default in testing.\n",
    "- QBART can scale across several PYNQs via ethernet, yielding a linear speedup (speedup ~= Number of PYNQs/1)\n",
    "\n",
    "Alright, let's get to it!\n",
    "\n",
    "## Requirements:\n",
    "- A trained QNN that is constructed with layers.py in the QNN folder, then pickled with python2 to a pickle file.\n",
    "- This must be placed on the PYNQ, and you must edit the QNN path below so that QBART can find and work on it.\n",
    "- Image(s) must also be placed in a seperate folders, and you must set the image path accordingly.\n",
    "- You must also manually configure the configpart below, and setup static IPs for your additional PYNQs if you want to use distributed computing.\n",
    "\n",
    "Alright, with the requirements done, we do the following:\n",
    "1. Run all image classifications on QBART, and time it.\n",
    "2. Run all image classifications on a pure, correct CPU implementation, and time it.\n",
    "3. Check if both QBART and the CPU implementation agree. If both implementations agree on all image classifications, we know that the QBART implementation is correct.\n",
    "4. Present the results to the user."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Running all image classifications on QBART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open source libraries\n",
    "import time\n",
    "import copy\n",
    "import os\n",
    "\n",
    "# Custom functions for the project\n",
    "from qbart_helper import *\n",
    "from client import classification_client\n",
    "\n",
    "# Provided by course instructor (github: Maltanar)\n",
    "from QNN import *\n",
    "\n",
    "###########################################################################################################\n",
    "### USER INPUT SECTION, USER MUST SUBMIT VALUES OR \"None\" WHERE APPLICABLE\n",
    "###########################################################################################################\n",
    "\n",
    "\n",
    "qnn_path = \"gtsrb-w1a1.pickle\"         # Image directory, relative to where the notebook resides.\n",
    "image_dir = \"Images\"                   # Image directory, relative to where the notebook resides.\n",
    "image_limit = 100                   # Max amount of images to be inferenced, set None to inference all.\n",
    "image_channels = \"RGB\"                 # Must be specified in order. 'R', 'G' and 'B' combinations only.\n",
    "image_data_layout = \"rcC\"              # Must be specified, r = row, c = column, C = Channel\n",
    "\n",
    "qbart_data_layout = \"Crc\"              # Qbart assumes data to be in column major form.\n",
    "\n",
    "qnn_trained_channels = \"BGR\"           # The channel ordering that the qnn is trained to.\n",
    "qnn_trained_imsize_col = 32            # The expected column size of input images to the qnn.\n",
    "qnn_trained_imsize_row = 32            # The expected row size of input images to the qnn.\n",
    "\n",
    "# Cluster config\n",
    "aase = '192.168.1.7'\n",
    "bjorg = '192.168.1.4'\n",
    "gunn = '192.168.1.2'\n",
    "solfrid = '192.168.1.5'\n",
    "qbart_port = 64646\n",
    "\n",
    "# At least one server (localhost or remote) must be running, or we can't run.\n",
    "server_list = [('localhost', qbart_port), (aase, qbart_port), (bjorg, qbart_port), (gunn, qbart_port), (solfrid, qbart_port)] \n",
    "\n",
    "# Either specify image classes to get an easily readable name, or specify None to just get a category #.\n",
    "image_classes = ['20 Km/h', '30 Km/h', '50 Km/h', '60 Km/h', '70 Km/h', '80 Km/h', 'End 80 Km/h', '100 Km/h', '120 Km/h', 'No overtaking', 'No overtaking for large trucks', 'Priority crossroad', 'Priority road', 'Give way', 'Stop', 'No vehicles', 'Prohibited for vehicles with a permitted gross weight over 3.5t including their trailers, and for tractors except passenger cars and buses', 'No entry for vehicular traffic', 'Danger Ahead', 'Bend to left', 'Bend to right', 'Double bend (first to left)', 'Uneven road', 'Road slippery when wet or dirty', 'Road narrows (right)', 'Road works', 'Traffic signals', 'Pedestrians in road ahead', 'Children crossing ahead', 'Bicycles prohibited', 'Risk of snow or ice', 'Wild animals', 'End of all speed and overtaking restrictions', 'Turn right ahead', 'Turn left ahead', 'Ahead only', 'Ahead or right only', 'Ahead or left only', 'Pass by on right', 'Pass by on left', 'Roundabout', 'End of no-overtaking zone', 'End of no-overtaking zone for vehicles with a permitted gross weight over 3.5t including their trailers, and for tractors except passenger cars and buses']\n",
    "\n",
    "###########################################################################################################\n",
    "###########################################################################################################\n",
    "\n",
    "###########################################################################################################\n",
    "### MAIN METHOD, SHOULD BE KEPT RELATIVELY SIMPLE, DETAILS STORED AWAY IN HELPER FUNCTIONS\n",
    "###########################################################################################################\n",
    "print(\"QBART Notebook now running\")\n",
    "print(\"Loading images\")\n",
    "images = load_images(image_dir, image_limit, qnn_trained_imsize_col, qnn_trained_imsize_row, qbart_data_layout, qnn_trained_channels)\n",
    "print(\"Loading QNN\")\n",
    "qnn = load_qnn(qnn_path)\n",
    "\n",
    "print(\"Starting timer for classification\")\n",
    "qbart_starttime = time.time()\n",
    "# We send the images to the processing server (currently localhost, can later be localhost and others (each with\n",
    "# its separate thread here in main or in classification client.))\n",
    "print(\"Sending images and qnn to server(s) for classification.\")\n",
    "qbart_classifications = classification_client(qnn, copy.copy(images), server_list)\n",
    "print(\"All results received.\")\n",
    "qbart_classifications = [j for i in qbart_classifications for j in i] # We flatten the list we receive. A bit messy.\n",
    "qbart_endtime = time.time()\n",
    "print(\"Timer stopped.\")\n",
    "\n",
    "\n",
    "    \n",
    "###########################################################################################################\n",
    "###########################################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Running all image classifications on a CPU implementation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the code from qnn-inference-examples (GTSRB only)\n",
    "With some modifications in order to batch process images instead of one-at-a-time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cPickle as pickle\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from QNN import *\n",
    "from time import time\n",
    "from QNN.layers import *\n",
    "from qbart_helper import *\n",
    "\n",
    "print(\"Starting CPU implement run on notebook.\")\n",
    "print(\"Loading qnn pickle\")\n",
    "# Load the qnn pickle string\n",
    "qnn_unpickled = pickle.loads(qnn)\n",
    "\n",
    "# Tutorial code galore\n",
    "print(\"Starting timer.\")\n",
    "tutorial_start = time()\n",
    "qnn_classifications = []\n",
    "\n",
    "print(\"Classifying..\")\n",
    "for image in images:\n",
    "    qnn_classifications.append((image[0], np.argmax(predict(qnn_unpickled, image[1]))))\n",
    "    \n",
    "tutorial_stop = time()\n",
    "tutorial_time_total = tutorial_stop - tutorial_start\n",
    "print(\"We finished classifying. Stopping the clock.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Step 3: Simple implementation correctness testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if qnn_classifications == qbart_classifications:\n",
    "    print(\"The classification lists are identical, therefore qbart works correctly.\")\n",
    "    # TODO: Present time used here. Perhaps energy later as well?\n",
    "else:\n",
    "    print(\"There is a mismatch between the pure cpu classification and the qbart classification. There is an error somewhere.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Step 4: Presentation of results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Time used by QBART classification: \" + str(qbart_endtime-qbart_starttime))\n",
    "print(\"Time used by tutorial classification: \" + str(tutorial_time_total)) \n",
    "\n",
    "# Since everything is a-ok, we print the results and also write it to a file for easy usage elsewhere.\n",
    "print(\"Printing classifications and writing to results.txt\")\n",
    "\n",
    "\n",
    "results_file = open(\"results.csv\",\"wb\")\n",
    "\n",
    "for i in range(len(qbart_classifications)):\n",
    "    print(qbart_classifications[i][0], image_classes[qbart_classifications[i][1]])\n",
    "    results_file.write(str(qbart_classifications[i][0]) + \",\" + str(image_classes[qbart_classifications[i][1]]) + os.linesep)\n",
    "\n",
    "results_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
