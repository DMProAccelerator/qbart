{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# QBART: Minimal Viable Product Edition\n",
    "\n",
    "*Welcome to QBART, the Quantized, Bitserial, AcceleRaThor!*\n",
    "\n",
    "<img src=\"logo.png\",width=400,height=400>\n",
    "\n",
    "In this MVP-implementation, the QBART-team have prepared the following:\n",
    "- Two layers run on the FPGA (the actual accelerator): thresholding and fully connected. A padding unit is also available for the several layers.\n",
    "- All the other layers run on the Cortex A9s: pooling, convolution, sliding window.\n",
    "- We utilize little to no BRAM on the FPGA, as most IO is saved directly to DRAM, and we have no custom memory hierarchy for the FPGA, so memory performance is suboptimal.\n",
    "- We use the GTSRB-benchmark as the default in testing.\n",
    "\n",
    "All in all, it might not accelerate anything at all, so the MVP is more a proof of concept, while future iterations will actually make this faster than the actual implementation. \n",
    "\n",
    "Alright, let's get to it!\n",
    "\n",
    "Requirements:\n",
    "- A trained QNN that is pickled and formatted similarly as the GTSRB benchmark.\n",
    "- This must be placed on the PYNQ, and you must edit the QNN path below so that QBART can find and work on it.\n",
    "- Image(s) must also be placed in a seperate folders, and you must set the image path accordingly.\n",
    "\n",
    "Alright, with the requirements done, we do the following:\n",
    "1. Run all image classifications on QBART, and time it.\n",
    "2. Run all image classifications on a pure, correct CPU implementation, and time it.\n",
    "3. Check if both QBART and the CPU implementation agree. If both implementations agree on all image classifications, we know that the QBART implementation is correct.\n",
    "4. Present the results to the user.\n",
    "\n",
    "TODO(N35N0M): We should consider a parameter for choosing how many of the images we want to run. The GTSRB test set is quite large, and if running all of the images takes hours, then we should only run a small subset for implementation correctness testing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Running all image classifications on QBART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This folder currently has the following images: ['left.jpg', 'stop.jpg', 'right.jpg', '50.jpg']\n",
      "Starts to read files\n",
      "[array([[[255, 255, 253],\n",
      "        [255, 255, 253],\n",
      "        [255, 255, 255],\n",
      "        ..., \n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255]],\n",
      "\n",
      "       [[255, 255, 253],\n",
      "        [255, 255, 253],\n",
      "        [255, 255, 255],\n",
      "        ..., \n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255]],\n",
      "\n",
      "       [[255, 255, 251],\n",
      "        [255, 255, 253],\n",
      "        [255, 255, 255],\n",
      "        ..., \n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255]],\n",
      "\n",
      "       ..., \n",
      "       [[157, 168, 200],\n",
      "        [136, 147, 179],\n",
      "        [143, 156, 190],\n",
      "        ..., \n",
      "        [171, 184, 229],\n",
      "        [145, 158, 200],\n",
      "        [130, 139, 178]],\n",
      "\n",
      "       [[147, 159, 185],\n",
      "        [140, 152, 178],\n",
      "        [160, 174, 201],\n",
      "        ..., \n",
      "        [160, 172, 222],\n",
      "        [143, 155, 203],\n",
      "        [132, 140, 186]],\n",
      "\n",
      "       [[139, 151, 175],\n",
      "        [144, 156, 180],\n",
      "        [173, 187, 213],\n",
      "        ..., \n",
      "        [147, 158, 212],\n",
      "        [151, 163, 213],\n",
      "        [157, 165, 212]]], dtype=uint8), array([[[255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        ..., \n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255]],\n",
      "\n",
      "       [[255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        ..., \n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255]],\n",
      "\n",
      "       [[255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        ..., \n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255]],\n",
      "\n",
      "       ..., \n",
      "       [[255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        [255, 255, 255],\n",
      "        ..., \n",
      "        [134, 127, 111],\n",
      "        [135, 128, 112],\n",
      "        [135, 128, 112]],\n",
      "\n",
      "       [[255, 254, 255],\n",
      "        [255, 255, 255],\n",
      "        [254, 255, 255],\n",
      "        ..., \n",
      "        [132, 126, 110],\n",
      "        [133, 126, 110],\n",
      "        [133, 127, 111]],\n",
      "\n",
      "       [[255, 250, 255],\n",
      "        [253, 253, 255],\n",
      "        [251, 255, 253],\n",
      "        ..., \n",
      "        [130, 127, 112],\n",
      "        [133, 127, 113],\n",
      "        [132, 129, 114]]], dtype=uint8), array([[[ 85,  69,  72],\n",
      "        [ 85,  71,  70],\n",
      "        [ 88,  76,  64],\n",
      "        ..., \n",
      "        [148, 157, 162],\n",
      "        [110, 119, 124],\n",
      "        [ 88,  97, 102]],\n",
      "\n",
      "       [[ 84,  68,  71],\n",
      "        [ 89,  75,  74],\n",
      "        [ 96,  86,  76],\n",
      "        ..., \n",
      "        [133, 143, 145],\n",
      "        [100, 107, 113],\n",
      "        [ 77,  87,  89]],\n",
      "\n",
      "       [[ 85,  70,  75],\n",
      "        [ 98,  86,  86],\n",
      "        [113, 104,  95],\n",
      "        ..., \n",
      "        [ 98, 106, 108],\n",
      "        [ 75,  80,  84],\n",
      "        [ 58,  66,  68]],\n",
      "\n",
      "       ..., \n",
      "       [[149, 145, 120],\n",
      "        [136, 134, 109],\n",
      "        [115, 114,  94],\n",
      "        ..., \n",
      "        [134, 139, 119],\n",
      "        [132, 138, 126],\n",
      "        [115, 121, 111]],\n",
      "\n",
      "       [[142, 139, 120],\n",
      "        [135, 134, 114],\n",
      "        [125, 126, 112],\n",
      "        ..., \n",
      "        [138, 143, 123],\n",
      "        [133, 139, 127],\n",
      "        [116, 122, 112]],\n",
      "\n",
      "       [[138, 136, 123],\n",
      "        [138, 139, 125],\n",
      "        [140, 143, 134],\n",
      "        ..., \n",
      "        [142, 147, 127],\n",
      "        [133, 139, 127],\n",
      "        [116, 122, 112]]], dtype=uint8), array([[[ 72,  94,  71],\n",
      "        [ 88, 107,  85],\n",
      "        [135, 152, 134],\n",
      "        ..., \n",
      "        [ 85,  84,  89],\n",
      "        [ 83,  74,  77],\n",
      "        [135, 123, 123]],\n",
      "\n",
      "       [[127, 145, 121],\n",
      "        [146, 161, 138],\n",
      "        [141, 154, 134],\n",
      "        ..., \n",
      "        [ 69,  69,  71],\n",
      "        [ 65,  57,  55],\n",
      "        [ 95,  84,  80]],\n",
      "\n",
      "       [[240, 250, 223],\n",
      "        [252, 255, 234],\n",
      "        [235, 240, 218],\n",
      "        ..., \n",
      "        [ 62,  61,  57],\n",
      "        [ 72,  63,  54],\n",
      "        [ 87,  75,  63]],\n",
      "\n",
      "       ..., \n",
      "       [[213, 177, 141],\n",
      "        [229, 195, 158],\n",
      "        [223, 193, 155],\n",
      "        ..., \n",
      "        [233, 212, 155],\n",
      "        [188, 157, 102],\n",
      "        [177, 140,  85]],\n",
      "\n",
      "       [[205, 169, 135],\n",
      "        [205, 171, 134],\n",
      "        [205, 175, 139],\n",
      "        ..., \n",
      "        [247, 222, 165],\n",
      "        [197, 162, 106],\n",
      "        [191, 150,  96]],\n",
      "\n",
      "       [[227, 192, 160],\n",
      "        [199, 166, 131],\n",
      "        [205, 175, 141],\n",
      "        ..., \n",
      "        [255, 233, 175],\n",
      "        [205, 166, 109],\n",
      "        [205, 160, 105]]], dtype=uint8)]\n",
      "[<QNN.layers.QNNConvolutionLayer object at 0x300265f0>, <QNN.layers.QNNBipolarThresholdingLayer object at 0x2ffa9030>, <QNN.layers.QNNPoolingLayer object at 0x2ffa9090>, <QNN.layers.QNNConvolutionLayer object at 0x2ffa90d0>, <QNN.layers.QNNBipolarThresholdingLayer object at 0x2ffa90f0>, <QNN.layers.QNNPoolingLayer object at 0x2ffa9110>, <QNN.layers.QNNFullyConnectedLayer object at 0x2ffa9150>, <QNN.layers.QNNScaleShiftLayer object at 0x2ffa9170>, <QNN.layers.QNNSoftmaxLayer object at 0x2ffa9190>]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "total size of new array must be unchanged",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mValueError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-cfa7e90b479f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0;31m# CONVOLUTION LAYER\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayerType\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"QNNConvolutionLayer\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m             \u001b[0mactivations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactivations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0;31m# FULLY CONNECTED LAYER\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/xilinx/jupyter_notebooks/MVP_Notebook/QNN/layers.py\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self, v)\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0mvn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 192\u001b[0;31m             \u001b[0mvn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    193\u001b[0m         \u001b[0;31m# reconstruct image matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m         \u001b[0mvn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mifm\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0modim\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0modim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/xilinx/jupyter_notebooks/MVP_Notebook/QNN/layers.py\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self, v)\u001b[0m\n\u001b[1;32m    155\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m         \u001b[0;31m# reshape the input vector into a 2D image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchans\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0midim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0midim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m         \u001b[0;31m# call im2col to get the sliding window result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m         res = im2col_indices(img, self.k, self.k, padding=0,\n",
      "\u001b[0;31mValueError\u001b[0m: total size of new array must be unchanged"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sys\n",
    "from QNN import *\n",
    "import pickle\n",
    "import os\n",
    "from time import sleep\n",
    "\n",
    "# CREDIT: Parts of the code are simply extracted from the tutorial code provided by the course instructors.\n",
    "# Should be available at: https://github.com/maltanar/qnn-inference-examples\n",
    "\n",
    "qnn_path = \"gtsrb-w1a1.pickle\" # This must be submitted by the user.\n",
    "image_dir = \"gtsrb_images\"  # This must be submitted by the user, relative to home folder.\n",
    "\n",
    "# Loads all the images from the folder provided by the user, \n",
    "# returns a python list of references to those images loaded as PIL images.\n",
    "# Should be placed in a separate function file when debug is finished.\n",
    "def load_images(image_dir):\n",
    "    # If the image directory is empty, then we return a sentinel value to indicate an error.\n",
    "    if image_dir is None:\n",
    "        print(\"No image directory specified.\")\n",
    "        return -1\n",
    "    \n",
    "    # Else we simply read in files\n",
    "    images = []\n",
    "    \n",
    "    print(\"Starts to read files\")\n",
    "    files = os.listdir(image_dir)\n",
    "    \n",
    "    for image in files:\n",
    "        # listdir only returns file name, not the relative path. So the following doesn't look that nice.\n",
    "        name = \"\".join((image_dir, \"/\", image))\n",
    "        \n",
    "        img = Image.open(name) \n",
    "        img = np.asarray(img)\n",
    "        images.append(img)\n",
    "        \n",
    "                \n",
    "    return images\n",
    "                \n",
    "\n",
    "print(\"This folder currently has the following images: \" + str(os.listdir(image_dir)))\n",
    "\n",
    "# TODO: Add error handling in the case of -1. Maybe the function should just raise a fatal error and halt the program?\n",
    "images = load_images(image_dir)\n",
    "print(images)\n",
    "\n",
    "plt.imshow(images[1], cmap='gray')\n",
    "sleep(1)\n",
    "    \n",
    "# Loads the QNN from a python2 pickle file, and returns a list of the layers, identical to the tutorial code.\n",
    "def load_qnn(qnn_filepath):\n",
    "    if qnn_filepath is None:\n",
    "        return -1\n",
    "    # load the qnn\n",
    "    qnn = pickle.load(open(qnn_filepath, \"rb\"))\n",
    "    return qnn\n",
    "\n",
    "qnn = load_qnn(qnn_path)\n",
    "print(qnn)\n",
    "\n",
    "# TODO: Add distribution of images if Beowulf cluster \n",
    "# is to be implemented (utilizing several PYNQs to exploit some DLP (working on entire images)).\n",
    "# The work to be done should be load balanced between all PYNQs.\n",
    "#\n",
    "# Some quantitiative measuring should be done on the implementation, to see if image transfer overhead will\n",
    "# overweigh the DLP benefits.\n",
    "\n",
    "#def beowulf_distribute():\n",
    "    # TODO: Define something here.\n",
    "    \n",
    "\n",
    "# In our MVP implementation, we do not support inferencing several images at once on the same FPGA,\n",
    "# so inference is simply a matter of going through all the given images, one at a time. Layer for layer,\n",
    "# saving the final classification for each image. \n",
    "#\n",
    "# Basically the predict function from \"layers.py\", but will be heavily modified.\n",
    "\n",
    "qbart_classifications = []\n",
    "\n",
    "for image in images:\n",
    "    activations = image\n",
    "    \n",
    "    for layer in qnn:\n",
    "        # Each layer will either do calculations on the A9 or the FPGA.\n",
    "        # Everything that the FPGA is unable to do, simply runs on the CPU.\n",
    "        #\n",
    "        # It will initially look very similar to alot in the provided \"layers.py\", but should in the end be entirely\n",
    "        # different when FPGA implements are finished.\n",
    "        #\n",
    "        # If there is no FPGA-implement, we simply reuse the provided sw-implement code by calling the \"execute\"\n",
    "        # method for each layer object.\n",
    "        \n",
    "        # CONVOLUTION LAYER\n",
    "        if (layer.layerType() == \"QNNConvolutionLayer\"):\n",
    "            activations = layer.execute(activations)\n",
    "            \n",
    "        # FULLY CONNECTED LAYER\n",
    "        elif (layer.layerType() == \"QNNFullyConnectedLayer\"):\n",
    "            activations = layer.execute(activations)\n",
    "            \n",
    "        # POOLING LAYER\n",
    "        elif (layer.layerType() == \"QNNPoolingLayer\"):\n",
    "            activations = layer.execute(activations)\n",
    "            \n",
    "        # THRESHOLDING LAYER\n",
    "        elif (layer.layerType() == \"QNNThresholdingLayer\"):\n",
    "            activations = layer.execute(activations)\n",
    "        \n",
    "        # SCALESHIFT LAYER\n",
    "        elif (layer.layerType() == \"QNNScaleShiftLayer\"):\n",
    "            activations = layer.execute(activations)\n",
    "            \n",
    "        # PADDING LAYER\n",
    "        elif (layer.layerType() == \"QNNPaddingLayer\"):\n",
    "            activations = layer.execute(activations)\n",
    "            \n",
    "        # SLIDING WINDOW LAYER\n",
    "        elif (layer.layerType() == \"QNNSlidingWindowLayer\"):\n",
    "            activations = layer.execute(activations)\n",
    "            \n",
    "        # LINEAR LAYER\n",
    "        elif (layer.layerType() == \"QNNLinearLayer\"):\n",
    "            activations = layer.execute(activations)\n",
    "            \n",
    "        # SOFTMAX LAYER\n",
    "        elif (layer.layerType() == \"QNNSoftmaxLayer\"):\n",
    "            activations = layer.execute(activations)\n",
    "            \n",
    "        # ReLU LAYER\n",
    "        elif (layer.layerType() == \"QNNReLULayer\"):\n",
    "            activations = layer.execute(activations)\n",
    "            \n",
    "        # BIPOLAR THRESHOLDING LAYER (Can't this be replaced by the general thresholding layer?)\n",
    "        elif (layer.layerType() == \"QNNBipolarThresholdingLayer\"):\n",
    "            activations = layer.execute(activations)\n",
    "            \n",
    "        else:\n",
    "            # Raise error, we are asked to perform a layer operation we do not know.\n",
    "            raise ValueError(\"Invalid layer type.\")\n",
    "    \n",
    "    # After all the layers have been performed, the final classification should be extracted and saved.\n",
    "    # Preferably as a tuple with (image name, classification), to clearly label output data.\n",
    "    qbart_classifications.append(np.argmax(activations))\n",
    "    \n",
    "# After all the images have been classified, we collect the results from the rest of the Beowulf cluster, if it exists.\n",
    "\n",
    "# Then, when all results are done, we move on to the CPU implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Running all image classifications on a CPU implementation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Step 3: Implementation correctness testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Step 4: Presentation of results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
