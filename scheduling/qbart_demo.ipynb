{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QBART: A general QNN inference Accelerator\n",
    "\n",
    "*Welcome to QBART, the Quantized, Bitserial, AcceleRaTor!*\n",
    "\n",
    "<img src=\"logo.png\",width=400,height=400>\n",
    "\n",
    "In this MVP-implementation, the QBART-team have prepared the following:\n",
    "- Three layers run on the FPGA: Thresholding, Fully Connected, and Convolution.\n",
    "- All the other layers run on the Cortex A9s through this notebook: pooling, minimax, and ??\n",
    "- We utilize little to no BRAM on the FPGA, as most IO is saved directly to DRAM, and we have no custom memory hierarchy for the FPGA, so memory performance is suboptimal.\n",
    "- We use the GTSRB-benchmark as the default in testing.\n",
    "- QBART can scale across several PYNQs via ethernet, yielding a linear speedup (speedup ~= Number of PYNQs/1)\n",
    "\n",
    "Alright, let's get to it!\n",
    "\n",
    "## Requirements:\n",
    "- A trained QNN that is constructed with layers.py in the QNN folder, then pickled with python2 to a pickle file.\n",
    "- This must be placed on the PYNQ, and you must edit the QNN path below so that QBART can find and work on it.\n",
    "- Image(s) must also be placed in a seperate folders, and you must set the image path accordingly.\n",
    "- You must also manually configure the configpart below, and setup static IPs for your additional PYNQs if you want to use distributed computing.\n",
    "\n",
    "Alright, with the requirements done, we do the following:\n",
    "1. Run all image classifications on QBART, and time it.\n",
    "2. Run all image classifications on a pure, correct CPU implementation, and time it.\n",
    "3. Check if both QBART and the CPU implementation agree. If both implementations agree on all image classifications, we know that the QBART implementation is correct.\n",
    "4. Present the results to the user."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Running all image classifications on QBART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', '/usr/lib/python2.7', '/usr/lib/python2.7/plat-x86_64-linux-gnu', '/usr/lib/python2.7/lib-tk', '/usr/lib/python2.7/lib-old', '/usr/lib/python2.7/lib-dynload', '/home/kris/.local/lib/python2.7/site-packages', '/usr/local/lib/python2.7/dist-packages', '/usr/lib/python2.7/dist-packages', '/usr/local/lib/python2.7/dist-packages/IPython/extensions', '/home/kris/.ipython']\n",
      "QBART Notebook now running\n",
      "Loading images\n",
      "Loading QNN\n",
      "Starting timer for classification\n",
      "Sending images and qnn to server(s) for classification.\n",
      "Successfully connected to localhost\n",
      "Connected servers are now receiving and working on images.\n",
      "All results received.\n",
      "Timer stopped.\n"
     ]
    }
   ],
   "source": [
    "# Open source libraries\n",
    "from time import time\n",
    "import copy\n",
    "import os\n",
    "import sys\n",
    "from imagenet_classes import *\n",
    "\n",
    "# Custom functions for the project\n",
    "from qbart_helper import *\n",
    "from client import classification_client\n",
    "\n",
    "# Provided by course instructor (github: Maltanar)\n",
    "from QNN import *\n",
    "\n",
    "###########################################################################################################\n",
    "### USER INPUT SECTION, USER MUST SUBMIT VALUES OR \"None\" WHERE APPLICABLE\n",
    "###########################################################################################################\n",
    "\n",
    "\"\"\"\n",
    "############## \n",
    "# MNIST Config\n",
    "##############\n",
    "qnn_path = \"/home/kris/Development/QBART_user_files/mnist-w1a2.pickle\"\n",
    "image_dir = \"/home/kris/Development/QBART_user_files/mnist_images\"\n",
    "image_limit = 10\n",
    "image_channels = \"grayscale\"\n",
    "image_data_layout = \"Crc\"\n",
    "\n",
    "qbart_data_layout = \"Crc\"\n",
    "\n",
    "qnn_trained_channels = \"grayscale\"\n",
    "qnn_trained_imsize_col = 28\n",
    "qnn_trained_imsize_row = 28\n",
    "qnn_class = \"MNIST\"\n",
    "\n",
    "image_classes = [\"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\"]\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "############## \n",
    "# GTSRB Config\n",
    "##############\n",
    "qnn_path = \"gtsrb-w1a1.pickle\"         # Image directory, relative to where the notebook resides.\n",
    "image_dir = \"Images\"                   # Image directory, relative to where the notebook resides.\n",
    "image_limit = 10                      # Max amount of images to be inferenced, set float(\"Inf\") to inference all.\n",
    "image_channels = \"RGB\"                 # Must be specified in order if the image is not a .jpg or .ppm\n",
    "image_data_layout = \"rcC\"              # Must be specified, r = row, c = column, C = Channel\n",
    "\n",
    "qbart_data_layout = \"Crc\"              # Qbart assumes data to be in column major form.\n",
    "\n",
    "qnn_trained_channels = \"BGR\"           # The color channel ordering that the qnn is trained to.\n",
    "qnn_trained_imsize_col = 32            # The expected column size of input images to the qnn.\n",
    "qnn_trained_imsize_row = 32            # The expected row size of input images to the qnn.\n",
    "qnn_class = \"GTSRB\"\n",
    "\n",
    "# Either specify image classes to get an easily readable name, or specify None to just get a category #.\n",
    "image_classes = ['20 Km/h', '30 Km/h', '50 Km/h', '60 Km/h', '70 Km/h', '80 Km/h', 'End 80 Km/h', '100 Km/h', '120 Km/h', 'No overtaking', 'No overtaking for large trucks', 'Priority crossroad', 'Priority road', 'Give way', 'Stop', 'No vehicles', 'Prohibited for vehicles with a permitted gross weight over 3.5t including their trailers, and for tractors except passenger cars and buses', 'No entry for vehicular traffic', 'Danger Ahead', 'Bend to left', 'Bend to right', 'Double bend (first to left)', 'Uneven road', 'Road slippery when wet or dirty', 'Road narrows (right)', 'Road works', 'Traffic signals', 'Pedestrians in road ahead', 'Children crossing ahead', 'Bicycles prohibited', 'Risk of snow or ice', 'Wild animals', 'End of all speed and overtaking restrictions', 'Turn right ahead', 'Turn left ahead', 'Ahead only', 'Ahead or right only', 'Ahead or left only', 'Pass by on right', 'Pass by on left', 'Roundabout', 'End of no-overtaking zone', 'End of no-overtaking zone for vehicles with a permitted gross weight over 3.5t including their trailers, and for tractors except passenger cars and buses']\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "############## \n",
    "# ImageNet Config\n",
    "##############\n",
    "# TODO: I think load_images and everything is set. Now we must just set path to qnn and images, and then try to run the darn thing.\n",
    "# Some work with the result array is required as well. Use qnn_class to set a custom classifying translation there.\n",
    "qnn_path = \"/home/kris/Development/QBART_user_files/alexnet-hwgq.pickle\"\n",
    "image_dir = \"/home/kris/Development/QBART_user_files/imagenet_minortest_images\"\n",
    "image_limit = 5\n",
    "image_channels = \"RGB\"\n",
    "image_data_layout = \"rcC\"\n",
    "\n",
    "qbart_data_layout = \"Crc\"\n",
    "\n",
    "qnn_trained_channels = \"BGR\"\n",
    "qnn_trained_imsize_col = 227\n",
    "qnn_trained_imsize_row = 227\n",
    "\n",
    "qnn_class = \"imagenet\"\n",
    "\n",
    "image_classes = imagenet_classes\n",
    "\n",
    "\n",
    "# Cluster config\n",
    "aase = '192.168.1.7'\n",
    "bjorg = '192.168.1.4'\n",
    "gunn = '192.168.1.2'\n",
    "solfrid = '192.168.1.5'\n",
    "qbart_port = 64646\n",
    "\n",
    "# At least one server (localhost or remote) must be running, or we can't run.\n",
    "server_list = [('localhost', qbart_port)] \n",
    "\n",
    "\n",
    "###########################################################################################################\n",
    "###########################################################################################################\n",
    "\n",
    "###########################################################################################################\n",
    "### MAIN METHOD, SHOULD BE KEPT RELATIVELY SIMPLE, DETAILS STORED AWAY IN HELPER FUNCTIONS\n",
    "###########################################################################################################\n",
    "print(\"QBART Notebook now running\")\n",
    "print(\"Loading images\")\n",
    "images = load_images(image_dir, image_limit, qnn_trained_imsize_col, qnn_trained_imsize_row, qbart_data_layout, qnn_trained_channels, qnn_class)\n",
    "print(\"Loading QNN\")\n",
    "qnn = load_qnn(qnn_path)\n",
    "\n",
    "print(\"Starting timer for classification\")\n",
    "qbart_starttime = time()\n",
    "# We send the images to the processing server (currently localhost, can later be localhost and others (each with\n",
    "# its separate thread here in main or in classification client.))\n",
    "print(\"Sending images and qnn to server(s) for classification.\")\n",
    "qbart_classifications = classification_client(qnn, copy.copy(images), server_list)\n",
    "print(\"All results received.\")\n",
    "qbart_classifications = [j for i in qbart_classifications for j in i] # We flatten the list we receive. A bit messy.\n",
    "qbart_endtime = time()\n",
    "print(\"Timer stopped.\")\n",
    "\n",
    "\n",
    "    \n",
    "###########################################################################################################\n",
    "###########################################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Running all image classifications on a CPU implementation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the code from qnn-inference-examples (GTSRB only)\n",
    "With some modifications in order to batch process images instead of one-at-a-time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting CPU implement run on notebook.\n",
      "Loading qnn pickle\n",
      "Starting timer.\n",
      "Classifying..\n",
      "We finished classifying. Clock stopped.\n"
     ]
    }
   ],
   "source": [
    "import cPickle as pickle\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "#from QNN import *\n",
    "from QNN.layers import *\n",
    "#from qbart_helper import *\n",
    "\n",
    "print(\"Starting CPU implement run on notebook.\")\n",
    "print(\"Loading qnn pickle\")\n",
    "# Load the qnn pickle string\n",
    "qnn_unpickled = pickle.loads(qnn)\n",
    "\n",
    "# Tutorial code galore\n",
    "print(\"Starting timer.\")\n",
    "tutorial_start = time()\n",
    "qnn_classifications = []\n",
    "\n",
    "print(\"Classifying..\")\n",
    "for image in images:\n",
    "    qnn_classifications.append((image[0], np.argmax(predict(qnn_unpickled, image[1]))))\n",
    "    \n",
    "tutorial_stop = time()\n",
    "tutorial_time_total = tutorial_stop - tutorial_start\n",
    "print(\"We finished classifying. Clock stopped.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Step 3: Simple implementation correctness testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "QBART is a QNN inference accelerator. Therefore, we do not test to see if the images are actually classified correctly, only that we execute the inference correctly. The actual classification accuracy is determined by the way that the QNN is trained. Therefore, we test for correctness by seeing if the classification list of the pure CPU classifications is equal to the QBART classifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The classification lists are identical, therefore qbart works correctly.\n"
     ]
    }
   ],
   "source": [
    "if qnn_classifications == qbart_classifications:\n",
    "    print(\"The classification lists are identical, therefore qbart works correctly.\")\n",
    "else:\n",
    "    print(\"There is a mismatch between the pure cpu classification and the qbart classification. There is an error somewhere.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Step 4: Presentation of classification results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time used by QBART classification: 7.89345216751\n",
      "Time used by tutorial classification: 7.16517496109\n",
      "Printing classifications and writing to results.txt\n",
      "('cat.jpg', 'tiger cat')\n",
      "('grouse.jpg', 'black grouse')\n",
      "('husky.jpg', 'Eskimo dog, husky')\n"
     ]
    }
   ],
   "source": [
    "print(\"Time used by QBART classification: \" + str(qbart_endtime-qbart_starttime))\n",
    "print(\"Time used by tutorial classification: \" + str(tutorial_time_total)) \n",
    "\n",
    "# Since everything is a-ok, we print the results and also write it to a file for easy usage elsewhere.\n",
    "print(\"Printing classifications and writing to results.txt\")\n",
    "\n",
    "\n",
    "results_file = open(\"results.csv\",\"wb\")\n",
    "\n",
    "# Here we print the image file name alongside its classification (a number if image_classes is not specified)\n",
    "# The result is also saved as a results.csv file.\n",
    "for i in range(len(qbart_classifications)):\n",
    "    if image_classes is not None:\n",
    "        print(qbart_classifications[i][0], image_classes[qbart_classifications[i][1]])\n",
    "        results_file.write(str(qbart_classifications[i][0]) + \",\" + str(image_classes[qbart_classifications[i][1]])+ os.linesep)\n",
    "    else:\n",
    "        print(qbart_classifications[i][0], qbart_classifications[i][1])\n",
    "        results_file.write(str(qbart_classifications[i][0]) + \",\" + str(qbart_classifications[i][1])+ os.linesep)\n",
    "results_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Benchmarking GEMMBitserial component on the FPGA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The runtimes of a single QBART vs the bare cpu-implementation do not really differ that much.\n",
    "FPGA layer execution is substantially faster than on numpy-methods on the CPU.\n",
    "\n",
    "A major bottleneck is sadly the amount of DRAM read/writes. In our current implementation, in order to support generality, we have to write data from CPU to RAM, and then from RAM to FPGA component before execution. Then, when results are done, we have to write data from FPGA to RAM, and from RAM to CPU. This adds a substantial data transfer overhead to the total execution time.\n",
    "\n",
    "To display how much potential there are in the FPGA components, we will now run a small benchmark, showing the difference in execution time when it comes to CPU execution and FPGA execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"PATH TO CFFI_RUN HERE PL0X\")\n",
    "\n",
    "from cffi_run import test_fc_benchmarks\n",
    "from bitserial_performance_model import FC_time\n",
    "\n",
    "# First we time a run of fully connected all the way from this notebook.\n",
    "# TODO: Load QNN pickle, go straight to the thresholding layer, and execute it with an Activation matrix that\n",
    "# has the right size. If the QNN is constant, then the activation matrix size will be too.\n",
    "\n",
    "\n",
    "# Before benchmarking the component, we calculate how many cycles it will take given our performance model.\n",
    "# FC_time expects FREQ WORD_SIZE COLS LHS_ROWS RHS_ROWS\n",
    "performance_model_predicted_time = FC_time()\n",
    "\n",
    "# Then we benchmark a FC Connected layer with similar parameters, the time outputted is the time on FPGA only.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
