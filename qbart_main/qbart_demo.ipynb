{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# QBART: A general QNN inference Accelerator\n",
    "\n",
    "*Welcome to QBART, the Quantized, Bitserial, AcceleRaTor!*\n",
    "\n",
    "<img src=\"logo.png\",width=400,height=400>\n",
    "\n",
    "In this MVP-implementation, the QBART-team have prepared the following:\n",
    "- Three layers run on the FPGA: Thresholding, Fully Connected, and Convolution.\n",
    "- All the other layers run on the Cortex A9s through this notebook: pooling, minimax, and ??\n",
    "- We utilize little to no BRAM on the FPGA, as most IO is saved directly to DRAM, and we have no custom memory hierarchy for the FPGA, so memory performance is suboptimal.\n",
    "- We use the GTSRB-benchmark as the default in testing.\n",
    "- QBART can scale across several PYNQs via ethernet, yielding a linear speedup (speedup ~= Number of PYNQs/1)\n",
    "\n",
    "Alright, let's get to it!\n",
    "\n",
    "## Requirements:\n",
    "- A trained QNN that is constructed with layers.py in the QNN folder, then pickled with python2 to a pickle file.\n",
    "- This must be placed on the PYNQ, and you must edit the QNN path below so that QBART can find and work on it.\n",
    "- Image(s) must also be placed in a seperate folders, and you must set the image path accordingly.\n",
    "- You must also manually configure the configpart below, and setup static IPs for your additional PYNQs if you want to use distributed computing.\n",
    "\n",
    "Alright, with the requirements done, we do the following:\n",
    "1. Run all image classifications on QBART, and time it.\n",
    "2. Run all image classifications on a pure, correct CPU implementation, and time it.\n",
    "3. Check if both QBART and the CPU implementation agree. If both implementations agree on all image classifications, we know that the QBART implementation is correct.\n",
    "4. Present the results to the user."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Step 1: Running all image classifications on QBART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QBART Notebook now running\n",
      "Loading images\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[Errno 2] No such file or directory: 'Images'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mOSError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-3fe97649c926>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"QBART Notebook now running\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Loading images\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_images\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_limit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqnn_trained_imsize_col\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqnn_trained_imsize_row\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqbart_data_layout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqnn_trained_channels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqnn_class\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Loading QNN\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0mqnn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_qnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_qnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqnn_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/xilinx/jupyter_notebooks/qbart_main/qbart_helper/load_images.pyc\u001b[0m in \u001b[0;36mload_images\u001b[0;34m(image_dir, no_images, colsize, rowsize, data_layout, channel_order, qnn_type)\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;31m# a warning to logger for the rest.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mfiles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mimage\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 2] No such file or directory: 'Images'"
     ]
    }
   ],
   "source": [
    "# Open source libraries\n",
    "from time import time\n",
    "import copy\n",
    "import os\n",
    "import sys\n",
    "from qbart_helper.imagenet_classes import *\n",
    "\n",
    "\n",
    "# Custom functions for the project\n",
    "from qbart_helper import load_images, load_qnn, qbart_socket_functions\n",
    "from qbart_helper.client import classification_client\n",
    "\n",
    "# Provided by course instructor (github: Maltanar)\n",
    "from qbart_helper.QNN import *\n",
    "\n",
    "###########################################################################################################\n",
    "### USER INPUT SECTION, USER MUST SUBMIT VALUES OR \"None\" WHERE APPLICABLE\n",
    "###########################################################################################################\n",
    "\n",
    "\"\"\"\n",
    "############## \n",
    "# MNIST Config\n",
    "##############\n",
    "qnn_path = \"/home/kris/Development/QBART_user_files/mnist-w1a2.pickle\"\n",
    "image_dir = \"/home/kris/Development/QBART_user_files/mnist_images\"\n",
    "image_limit = 10\n",
    "image_channels = \"grayscale\"\n",
    "image_data_layout = \"Crc\"\n",
    "\n",
    "qbart_data_layout = \"Crc\"\n",
    "\n",
    "qnn_trained_channels = \"grayscale\"\n",
    "qnn_trained_imsize_col = 28\n",
    "qnn_trained_imsize_row = 28\n",
    "qnn_class = \"MNIST\"\n",
    "\n",
    "image_classes = [\"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\"]\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "############## \n",
    "# GTSRB Config\n",
    "##############\n",
    "qnn_path = \"qbart_user_files/gtsrb-w1a1.pickle\"         # Image directory, relative to where the notebook resides.\n",
    "image_dir = \"qbart_user_files/gtsrb_images\"                   # Image directory, relative to where the notebook resides.\n",
    "image_limit = 10                      # Max amount of images to be inferenced, set float(\"Inf\") to inference all.\n",
    "image_channels = \"RGB\"                 # Must be specified in order if the image is not a .jpg or .ppm\n",
    "image_data_layout = \"rcC\"              # Must be specified, r = row, c = column, C = Channel\n",
    "\n",
    "qbart_data_layout = \"Crc\"              # Qbart assumes data to be in column major form.\n",
    "\n",
    "qnn_trained_channels = \"BGR\"           # The color channel ordering that the qnn is trained to.\n",
    "qnn_trained_imsize_col = 32            # The expected column size of input images to the qnn.\n",
    "qnn_trained_imsize_row = 32            # The expected row size of input images to the qnn.\n",
    "qnn_class = \"GTSRB\"\n",
    "\n",
    "# Either specify image classes to get an easily readable name, or specify None to just get a category #.\n",
    "image_classes = ['20 Km/h', '30 Km/h', '50 Km/h', '60 Km/h', '70 Km/h', '80 Km/h', 'End 80 Km/h', '100 Km/h', '120 Km/h', 'No overtaking', 'No overtaking for large trucks', 'Priority crossroad', 'Priority road', 'Give way', 'Stop', 'No vehicles', 'Prohibited for vehicles with a permitted gross weight over 3.5t including their trailers, and for tractors except passenger cars and buses', 'No entry for vehicular traffic', 'Danger Ahead', 'Bend to left', 'Bend to right', 'Double bend (first to left)', 'Uneven road', 'Road slippery when wet or dirty', 'Road narrows (right)', 'Road works', 'Traffic signals', 'Pedestrians in road ahead', 'Children crossing ahead', 'Bicycles prohibited', 'Risk of snow or ice', 'Wild animals', 'End of all speed and overtaking restrictions', 'Turn right ahead', 'Turn left ahead', 'Ahead only', 'Ahead or right only', 'Ahead or left only', 'Pass by on right', 'Pass by on left', 'Roundabout', 'End of no-overtaking zone', 'End of no-overtaking zone for vehicles with a permitted gross weight over 3.5t including their trailers, and for tractors except passenger cars and buses']\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "############## \n",
    "# ImageNet Config\n",
    "##############\n",
    "# TODO: I think load_images and everything is set. Now we must just set path to qnn and images, and then try to run the darn thing.\n",
    "# Some work with the result array is required as well. Use qnn_class to set a custom classifying translation there.\n",
    "qnn_path = \"/home/kris/Development/QBART_user_files/alexnet-hwgq.pickle\"\n",
    "image_dir = \"/home/kris/Development/QBART_user_files/imagenet_minortest_images\"\n",
    "image_limit = 5\n",
    "image_channels = \"RGB\"\n",
    "image_data_layout = \"rcC\"\n",
    "\n",
    "qbart_data_layout = \"Crc\"\n",
    "\n",
    "qnn_trained_channels = \"BGR\"\n",
    "qnn_trained_imsize_col = 227\n",
    "qnn_trained_imsize_row = 227\n",
    "\n",
    "qnn_class = \"imagenet\"\n",
    "\n",
    "image_classes = imagenet_classes\n",
    "\"\"\"\n",
    "\n",
    "# Cluster config\n",
    "aase = '192.168.1.7'\n",
    "bjorg = '192.168.1.4'\n",
    "gunn = '192.168.1.2'\n",
    "solfrid = '192.168.1.5'\n",
    "qbart_port = 64646\n",
    "\n",
    "# At least one server (localhost or remote) must be running, or we can't run.\n",
    "server_list = [('localhost', qbart_port), (gunn, qbart_port)] \n",
    "\n",
    "\n",
    "###########################################################################################################\n",
    "###########################################################################################################\n",
    "\n",
    "###########################################################################################################\n",
    "### MAIN METHOD, SHOULD BE KEPT RELATIVELY SIMPLE, DETAILS STORED AWAY IN HELPER FUNCTIONS\n",
    "###########################################################################################################\n",
    "print(\"QBART Notebook now running\")\n",
    "print(\"Loading images\")\n",
    "images = load_images.load_images(image_dir, image_limit, qnn_trained_imsize_col, qnn_trained_imsize_row, qbart_data_layout, qnn_trained_channels, qnn_class)\n",
    "print(\"Loading QNN\")\n",
    "qnn = load_qnn.load_qnn(qnn_path)\n",
    "\n",
    "print(\"Starting timer for classification\")\n",
    "qbart_starttime = time()\n",
    "# We send the images to the processing server (currently localhost, can later be localhost and others (each with\n",
    "# its separate thread here in main or in classification client.))\n",
    "print(\"Sending images and qnn to server(s) for classification.\")\n",
    "qbart_classifications = classification_client(qnn, copy.copy(images), server_list)\n",
    "print(\"All results received.\")\n",
    "qbart_classifications = [j for i in qbart_classifications for j in i] # We flatten the list we receive. A bit messy.\n",
    "qbart_endtime = time()\n",
    "print(\"Timer stopped.\")\n",
    "\n",
    "\n",
    "    \n",
    "###########################################################################################################\n",
    "###########################################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Step 2: Running all image classifications on a CPU implementation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Using the code from qnn-inference-examples (GTSRB only)\n",
    "With some modifications in order to batch process images instead of one-at-a-time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting CPU implement run on notebook.\n",
      "Loading qnn pickle\n",
      "Starting timer.\n",
      "Classifying..\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-490de3fe1ed2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Classifying..\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mimage\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0mqnn_classifications\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqnn_unpickled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0mtutorial_stop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/xilinx/jupyter_notebooks/qbart/scheduling/QNN/layers.pyc\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(qnn, input_img)\u001b[0m\n\u001b[1;32m     50\u001b[0m   \u001b[0mactivations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_img\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mqnn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m     \u001b[0mactivations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactivations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mactivations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/xilinx/jupyter_notebooks/qbart/scheduling/QNN/layers.pyc\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self, v)\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;31m# LinearLayer, but this way we keep the bipolar thresholding as a stand-\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;31m# alone operation.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mQNNBipolarThresholdingLayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mret\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/xilinx/jupyter_notebooks/qbart/scheduling/QNN/layers.pyc\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self, v)\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthresholds\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthresholds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m                 \u001b[0mret\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mTrue\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import cPickle as pickle\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "#from QNN import *\n",
    "from QNN.layers import *\n",
    "#from qbart_helper import *\n",
    "\n",
    "print(\"Starting CPU implement run on notebook.\")\n",
    "print(\"Loading qnn pickle\")\n",
    "# Load the qnn pickle string\n",
    "qnn_unpickled = pickle.loads(qnn)\n",
    "\n",
    "# Tutorial code galore\n",
    "print(\"Starting timer.\")\n",
    "tutorial_start = time()\n",
    "qnn_classifications = []\n",
    "\n",
    "print(\"Classifying..\")\n",
    "for image in images:\n",
    "    qnn_classifications.append((image[0], np.argmax(predict(qnn_unpickled, image[1]))))\n",
    "    \n",
    "tutorial_stop = time()\n",
    "tutorial_time_total = tutorial_stop - tutorial_start\n",
    "print(\"We finished classifying. Clock stopped.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Step 3: Simple implementation correctness testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "QBART is a QNN inference accelerator. Therefore, we do not test to see if the images are actually classified correctly, only that we execute the inference correctly. The actual classification accuracy is determined by the way that the QNN is trained. Therefore, we test for correctness by seeing if the classification list of the pure CPU classifications is equal to the QBART classifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The classification lists are identical, therefore qbart works correctly.\n"
     ]
    }
   ],
   "source": [
    "if qnn_classifications == qbart_classifications:\n",
    "    print(\"The classification lists are identical, therefore qbart works correctly.\")\n",
    "else:\n",
    "    print(\"There is a mismatch between the pure cpu classification and the qbart classification. There is an error somewhere.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Step 4: Presentation of results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time used by QBART classification: 7.89345216751\n",
      "Time used by tutorial classification: 7.16517496109\n",
      "Printing classifications and writing to results.txt\n",
      "('cat.jpg', 'tiger cat')\n",
      "('grouse.jpg', 'black grouse')\n",
      "('husky.jpg', 'Eskimo dog, husky')\n"
     ]
    }
   ],
   "source": [
    "print(\"Time used by QBART classification: \" + str(qbart_endtime-qbart_starttime))\n",
    "print(\"Time used by tutorial classification: \" + str(tutorial_time_total)) \n",
    "\n",
    "# Since everything is a-ok, we print the results and also write it to a file for easy usage elsewhere.\n",
    "print(\"Printing classifications and writing to results.txt\")\n",
    "\n",
    "\n",
    "results_file = open(\"results.csv\",\"wb\")\n",
    "\n",
    "# Here we print the image file name alongside its classification (a number if image_classes is not specified)\n",
    "# The result is also saved as a results.csv file.\n",
    "for i in range(len(qbart_classifications)):\n",
    "    if image_classes is not None:\n",
    "        print(qbart_classifications[i][0], image_classes[qbart_classifications[i][1]])\n",
    "        results_file.write(str(qbart_classifications[i][0]) + \",\" + str(image_classes[qbart_classifications[i][1]])+ os.linesep)\n",
    "    else:\n",
    "        print(qbart_classifications[i][0], qbart_classifications[i][1])\n",
    "        results_file.write(str(qbart_classifications[i][0]) + \",\" + str(qbart_classifications[i][1])+ os.linesep)\n",
    "results_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
